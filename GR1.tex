\documentclass[26pt,fleqn,]{article}
%\usepackage[]{amsmath}
\usepackage[]{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{xytree}
\usepackage{enumitem}
\date{}
\title{Graduation Research 2 Report\\
Linguistic Logic Reasoning}
\author{Instructor: Tran Duc Khanh\\Student: Ngo Nhat Anh, ID: 20090102\\}
\begin {document}
%\large
\maketitle
%test
%\begin{align}
%e^i\pi\\ A \in B \\B \ni A \\A \not\ni B \\M > N \\\bar{A} \equiv B \cap \bar{C}
%\end{align}
%endtest
%
\section{Hedge algebra}
\subsection{Definition}
{\bfseries Linguistic hedge:} a unary operation on linguistic value, changing the linguistic value's meaning.
\\
{\bfseries Hedge algebra:} Let \(X\) be a linguistic variable and its domain is \(X = Dom(X)\). A hedge 
algebra \(AX\) in respect to \(X\) is a 4-tuple \(AX = (X,G,H,\le)\) where G is the set of spanning
elements, H is the set of hedges and \(\le\) is the sematic ordering relation on X.
The set G usually has positive, negative and neutral spanning elements. In practice, G usually has only
positive and negative spanning elements e.g \{True,False\}, \{High, Low\}.\\

A hedge algebra AX satisfies the following  axioms:\\
{\indent
	(1) Hedges either increase or decrease the effect of other hedges including itself, and it is 
then positive or negative w.r.t. the other.\\}
{\indent
	(2) If \(u\notin H(v)\) and \(v\notin H(u)\) then \(\forall x\in H(u), x\notin H(v)\) and vice versa. 
	Furthermore if u and v cannot be compared then so are x,y for all \(x\in H(u), y\in H(v)\).
}\\
{\indent
	(3) \(x \not = hx, x \notin H(hx). h\neq k, hx<kx\) then \(h'hx\le k'kx\forall h,h',k,k'\in
	H.\) Furthermore, if \(hx \neq kx\) then hx and kx are independent w.r.t. to each other.
}\\
{\indent
	(4) \(u \in v, u \le v\implies u\le hv\) for \(\forall h\) and vice versa.
}
\\\\
(1) means exactly what it says.\\
(2) means that if two vague concepts are really different, then they form seperated concept categories,
which means they don't have any common meaning.\\
(3) means that each hedge has its own meaning and defines its own concept category.\\
(4) means that a hedge only modifies the vague concept's meaning. It preserves the ordering relation
of the vague concept's meaning w.r.t. other vague concepts.\\
\subsection{Properties}
	{\bfseries Semantic heredity:} When a hedge affects the meaning of a linguistic value, it only
	increases or decreases a little bit the meaning of that value. The resulting value inherits
	most of its parent's meaning i.e the parent's comparability is preserved: 
	if hx \(\le\) kx then H(hx) \(\le\) H(kx). Generally, \(\forall (u,v \in X, 
	\sigma_1 = h_n..h_1, \sigma_2 = k_m..k_1, h_i,k_j \in H)\), we have:
	\(u \le v \implies \sigma_1u \le \sigma_2v\)\\
	{\bfseries Theorem 1:} Let AX = (X,G,H,\(\le\)). The following statements hold:\\
\indent (1) If x \(\in\) X is a fixed point of an operation h in H, i.e. hx = x, then it is a fixed
	 point of the others.\\
\indent (2) If x = \(h_n \cdots h_1 u\), and there exists an index \(i<n\) such that \(h_i \cdots h_1 u\)        is another representation (canonical) of x w.r.t. u and \(h_j x = x, \forall j\ge i\).\\
\indent (3) For any \(h, k\in H\), if \(hx\le kx\) and \(h\neq k\) then \(hx< kx\).\\
\indent \(\implies\) The canonical representation of any linguistic value w.r.t. another value is unique.	\\
\indent {\bfseries Theorem 2:} Let \(x = h_n \cdots h_1 u\) and \(y = k_m \cdots k_1 u\) be two arbitrary
	canonical representations of x and y w.r.t. u, respectively. Then there exists an index
	\(j \le min{n, m} + 1\) such that \(h_i = k_i, \forall i < j\), and\\
\indent (1) \(x < y \iff h_j x_j < k_j x_j , where x_j = h_j-1 \cdots h_1 u\)\\
\indent (2) \(x = y \iff n = m = j and h_j x_j = k_j x_j\)\\
\indent (3) x and y are incomparable \(\iff h_j x_j\) and \(k_j x_j\) are incomparable\\
\indent \(\implies\) This theorem provides us the mean to compare two linguistic value in a hedge 
algebra. 

\subsection{Linear symmetrical algebra}
The spanning set G usually has two comparable linguistic values. For example, we have False \(<\) True
for truth value. A hedge algebra that has only two primitive linguistic values is called a symmetrical
hedge algebra. The spanning set is denoted \(G = \{c^-, c^+\}, c^+, c^-\) are positive and negative 
spanning element, respectively.\\\\

The set H of hedges can also be partitioned into two seperated subsets \(H^+ = \{h|hc^+ > c^+\} = 
\{h|hc^- < c^-\}, H^- = \{h|hc^- > c^-\} = 
\{h|hc^+ < c^+\}\). Any two hedges in \(H^+\) (or \(H^-\)) are comparable to each other, and each hedge in \(H^+\) is converse to a hedge in \(H^-\) and vice versa. A hedge algebra \(AX = (X,\{c^+,c^-\},H^+\cup
H^-, \le)\) is linear symmetrical iff \(H^+\) and \(H^-\) are linearly ordered. It is easy to see
that X is also linearly ordered by \(\le\).\\\\

Let I \(\notin\) H be the {\em identity hedge}: \(\forall x \in X, Ix = x\). I is smaller than any other
hedge in \(H^+\) and \(H^-\).\\\\

If \(H \neq {\o}, H(c^+), H(c^=)\) are infinite then \(inf(c^+) = sup(c^-) = W, inf(c^-) = 0, sup(c^+) = 1\). W is call the neutral element. We have: \(0<c^-<W<c^+<1\). Denote the linguistic value domain as
\(\bar X = X \cup \{0,W,1\}\)\\\\

From that we define:
\begin{itemize}
\item Let \(x = \sigma c, \sigma \in H^*, c \in \{c^+, c^-\}. y \text{is the negation of x, denoted}
	y = -x \text{ if } y = \sigma c', \{c,c'\} = \{c^+, c^-\}. -0 = 1, -1 = 0, -W = W\).
\item \(x,y \in \bar X\) then \(x \wedge y = min(x,y), x \vee y = max(x,y)\)
\end{itemize}


\subsection{Rule of moving hedges}
\indent (RT1): \(\frac{((P,hu),\sigma <True|False>)} {((P,u),\sigma h<True|False>)}\)
\\
\\
	(RT2): \(\frac{((P,u),\sigma h<True|False>)} {((P,hu),\sigma <True|False>)}\)
\section{A propositional logic with truth value domain based on LSA}
\subsection{Syntax}
\indent {\bfseries The alphabet of logic} consists of the following classes of symbol:\\
\indent + Propositional symbols: A,B,C \ldots\\
\indent + Linguistic value symbols: a,b,c \ldots\\
\indent + Constant symbols: 0,1,W\ldots\\
\indent + Logical connectives:\(\vee,\wedge,\to,\neg\) \ldots\\
\indent + Auxiliary symbols: ',', '(', ')'\ldots\\ \\
{\bfseries Literal:} A string \(A^a\), where \(A\) is a propositional symbol and \(a\) is a truth
value in \(A\)'s domain.\\\\
 {\bfseries Formula:}\\
 \indent + A literal is a formula\\
 \indent + F is a formula then \( (\neg F\)) is also a formula\\
 \indent + F and G are formulae then \( (F\vee G), (F\wedge G), (F \to G)\) are also formulae\\
 \indent + Only strings generated by the above rules are formulae\\
 \indent ( {\bfseries Precedence of operations:} \(\neg\)  >>  \(\to\)  >>  \(\wedge\)  >>  \(\vee\))\\

\subsection{Semantic}
\subsubsection{Interpretation}
An {\bfseries Interpretation} I:\{A = a1\} of the literal \(A^{a2}\) is the mapping associate the formula
with a value of the truth value domain.\\

{\bfseries Logical connectives' semantics:} let a and b be some linguistic value. Then:\\
\indent + \(a \vee b = max(a,b)\)\\
\indent + \(a \wedge b = min(a,b)\)\\
\indent + \(\neg a = \text{symmetrical value of } a\)\\
\indent + \(a \to b = max(\neg a,b)\)\\
\(\implies \vee\) and \(\wedge\) are Goedel's norm and conorm.\\
Let T(A) be A's truth value under an arbitrary interpretation.\\
{\bfseries Truth value of formulae:}\\
\indent + If A is a literal, T(A) is determined by the interpretation\\
\indent + Let A and B be two formulae:\\
\indent \indent (1) \(T(A\vee B) = T(A) \vee T(B)\)\\
\indent \indent (2) \(T(A\wedge B) = T(A) \wedge T(B)\)\\
\indent \indent (3) \(T(A\to B) = T(A) \to T(B)\)\\
\indent \indent (4) \(T(A\neg B) = T(A) \neg T(B)\)
\\\\
Let \(T_{I:\{A = a1\}}(A^{a2})\) the value of that interpretation. Then:\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = a1 \wedge a2 \) if a1, a2 \(\ge W\)\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = \neg (a1 \vee a2) \) if a1, a2 \(<\) W\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = a1 \wedge \neg a2 \) if a1 \(\ge\) W, a2 \(<\) W\\
\indent + \(T_{I:\{A = a1\}}(A^{a2}) = \neg a1 \wedge a2 \) if a1 \(<\) W, a2 \(\ge\) W\\

\subsubsection{Satisfiable, unsatisfiable, false}
{\bfseries Definition:} Let S be a formula, I be an arbitrary interpretation:\\
\indent + S is false under I if \(T_I(S) < W\)\\
\indent + S is unsatisfiable if it is contradicted to every interpretation\\ 
\indent + S is satisfiable if \(\exists I: T_I(S) \ge W\). I is then called a model of S, denoted \(I \models S\)\\
\indent + S is a tautology if \(\forall I: T_I(S) \ge W\)\\
\indent \(\implies)\) Collorary: A formula A is a tautology iff \(\neg\)A is unsatisfiable.
\subsubsection{Logical equivalences. Functional complete set of logical connectives}
\indent Two formula A abd V are said to be equivalent (\(A \equiv B\)) if A and B have the same \\
indent truth value for every interpretation I.\\
\indent Some well-known logical equivalences:
\begin{align*}
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
+ A \to B \equiv \neg A \vee B\\
\end{align*}
\indent {\bfseries Functionally complete set of logical connectives} is one which can be used to express all possible logical functions \(f:TV^n \to TV\)\\
\subsubsection{Conjunctive normal form}
A formula expressed as a conjunction of formulae, where these formulae are in turn expressed as
a disjunction of literal, is said to be in {\em Conjunctive normal form}.\\
{\bfseries Theorem:} Every formula in our propositional logic is logically equivalent to a CNF
formula.\\
Algorithm to transform a formula into its CNF:\\\\
\indent- Eliminate \(\to\) and \(\leftrightarrow\) :
\begin{align*}
	&F \to G \underset{CNF}{\implies} \neg F \vee G\\
	&F \leftrightarrow G \underset{CNF}{\implies} (\neg F \vee G) \wedge (\neg G \vee F)
\end{align*}
\indent- Move \(\neg\) inward: 
\begin{align*}
	&\neg (F \vee G) \underset{CNF}{\implies} \neg F \wedge \neg G
\end{align*}
\indent- Elimitate double negation:
\begin{align*}
	&\neg \neg F \underset{CNF}{\implies} F
\end{align*}
\indent- Applying distributivity law:
\begin{align*}
	&F \wedge (G \vee H) \underset{CNF}{\implies} (F \wedge G) \vee (F \wedge H)\\
	&F \vee (G \wedge H) \underset{CNF}{\implies} (F \vee G) \wedge (F \vee H)
\end{align*}	
\indent- Rewrite redundant formula:
\begin{align*}	
	&F \vee F \underset{CNF}{\implies} F\\
	&F \wedge F \underset{CNF}{\implies} F
\end{align*}
\subsection{Inference}
\subsubsection{Logical consequence}
{\bfseries Logical consequence:} A is a logical consequence of B if every interpretation 
satisfying A also satisties B. Notated: A \(\models\) B.\\
\subsubsection{Inference rule:}
{\bfseries Inference rule:} An inference rule of the form\\
\begin{align*}
	\frac{F_1, F_2,\cdots,F_n}{G}
\end{align*}
means that with a given set of formula \(F_1,F_2,\cdots,F_n\), we can deduce a 
new formula \(G\). An inference rule is sound if it satisfies logical consequence.\\\\
To be useful, aside from being sound, an inference rule also should be complete, meaning\\
it deduce every formula that is a logical consequence of the original set of formulae.\\
\subsubsection{Confidence value}
In fuzzy logic, inference rules are only approxiamtely true, so formulae derived by them must be
taken with a {\em confidence value}. A formula F with \(\alpha\) confidence is denoted by \(F_\alpha\).\\\\

Clauses in the initial knowledge base are given with some confidence value. The confidence of formulae
derived from them will be determined by their given confidence value. Obviously the derived formula's 
confidence must not be larger than its original formulae's.
%
%
\section{Resolution in HA-based logic}
\subsection{Resolution rule}
{\bfseries Resolution rule:} Given \(A^a \vee B^{b1}\) and \(C^c \vee B^{b2}\). If \(b1 \vee b2 
\ge W\) and \(b1 \wedge b2 < W\):
\begin{align*}
	&\frac{(A^a \vee B^{b1})_{\alpha 1}, (C^c \vee B^{b2})_{\alpha 2}} {(A^a \vee C^c)_{\alpha 3}}\\
	&\text{Where: }
	\begin{cases}
	\alpha 1, \alpha 2, \alpha 3 \text{ are confidence value of formulae.}\\
	A, B, C \text{ are propositional symbols}\\
	a, b1, b2, c \text{ are linguistic value}
	\end{cases}
	\\
	&\text{If } (A^a \vee B^{b1})_{\alpha 1} \wedge (C^c \vee B^{b2})_{\alpha 2} \text{ is 
	unsatisfiable, the result of resolution is the null clause.} 
\end{align*}
\(\implies\) To be able to reasoning using resolution, we first have to transform the original
formulae into their CNFs.\\

{\bfseries Resolution algorithm:} 
 To prove that a goal clause G entails the knowledge base KB, e.g \(KB \models G\), we can instead prove
that \(KB \cup \neg G\) is unsatisfiable.\\\\
\indent {\em Input: S = \(KB \cup \neg G\)}\\
\indent {\em Output: S is unsatisfiable or not}\\\\
\indent{\bfseries BEGIN}\\
\indent If (S contains NULL) thenn return true;\\
\indent Foreach (clause C1 in S)\\
\indent \indent Foreach (clause C2 in S)\\
\indent \indent \indent If (C1 and C2 can be resolved) C3 := resolve(C1,C2);\\
\indent \indent \indent If (C3 = NULL) return true;\\
\indent \indent \indent Else (Add C3 to S);\\
\indent If (S does not contain NULL) then return false;\\
\indent{\bfseries END}\\


\subsection{Confidence value in HA-based logic}
By the definition of resolution, we can see for the two clauses to be able to resolve, \(b_1\) and 
\(b_2\) must be on different side w.r.t. W, and the bigger their difference, the more confident the 
inference. We consider 4 methods to determine the confidence value of an inference step.\\
\subsubsection{Confidence using min}
Obviously, the inference step is more certain as the smaller value of \(b_1\) and \(b_2\) is closer to 
0. In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, \neg(b_1 \wedge b_2))
\end{align*}
\subsubsection{Confidence using max}
Conversely, the inference step is more certain as the bigger value of \(b_1\) and \(b_2\) is closer to 
1. In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, (b_1 \vee b_2))
\end{align*}
\subsubsection{Confidence using max and min}
Each of the previous method can only express part of the confidence of the resolution. We can combine
them to get a confidence value that represent both the ``false'' and ``true'' values \(b_1\) and 
\(b_2\). In particular, the confidence value of the inferred clause is:
\begin{align*}
	{\alpha}_3 = \wedge({\alpha}_1, {\alpha}_2, (b_1 \vee b_2), \neg(b_1 \wedge b_2))
\end{align*}
\subsubsection{Confidence using the semantic difference of two value}
In the previous methods, we only consider how right (or wrong) \(b_1\) and \(b_2\) are to determine
the confidence of our inference. To better evaluate the confidence value, we must be able to express
the difference between \(b_1\) and \(b_2\), as the more different they are the more certain our 
inference is and vice versa.\\
\\
Assuming we have the indexing function \(h: X \to N\) mapping a linguistic value to the number of 
elements in X that is no larger than it. Notice that the number of elements in X must be finite then, 
which means the maximum length of hedge strings in HA must also be finite.\\ 
\\
The difference between to linguistic value x1 and x2 is then defined by:
\begin{align*}
	dif(x1,x2) = x, \text{ where: } h(x) = h(x1) - h(x2)\\
	\text{The confidence value is:}\\
	\alpha_3 = \wedge(\alpha_1,\alpha_2,dif(b_1,b_2))
\end{align*}
\\
This definition of confidence value is the most accurate out of the four. It clearly states the relation
between two ``opposing'' value \(b_1\) and \(b_2\). The only downside is that we have to limit the 
length of hedge strings to be able to calculate the difference. It is shown that there is an 
isomorphism between the set X of a LHA {X,G,H,\(\le\)} and the interval [0..1]. If we had a computable
method to map each value in X to a value in [0..1], we could calculate the difference even more 
accurately, and more importantly, without limitting the maximum length of hedge strings.\\
\subsection{Proof of Soundness}
Proof of soundness means that we have to prove: \(KB {\vdash}_{resolution} G \implies KB \models G\). To
put in English: the if resolution method can derive G from KB then G is satisfiable in every model of 
KB. We can instead prove that \(S = KB \cup \neg G\) is unsatisfiable if doing resolution on
S derives the null clause.\\

\paragraph{Lemma 1:} Let \(A^a \vee B^{b1}, C^c \vee B^{b2}\) be the input clauses of a resolution, and 
\(A^a \vee C^c\) is the result. If \(A^a \vee C^c\) is false under an interpretation I, then 
\(A^a \vee B^{b1}, C^c \vee B^{b2}\) is also false under I.\\
\indent Proof:
\begin{align*}
	&T((A^a \vee B^{b1}) \wedge (C^c \vee B^{b2}))\\
	&= T(A^a \wedge C^c) \vee T(A^a \wedge B^{b2}) \vee T(C^c \wedge B^{b1}) \vee T(B^{b1} \wedge 
	B^{b2})\\
	&\text{We have:}\\
	&W > T(A^a \vee C^c) \ge T(A^a \wedge C^c)\\
	&W > T(A^a \vee C^c) \ge T(A^a) \ge T(A^a \wedge B^{b2})\\
	&W > T(A^a \vee C^c) \ge T(C^c) \ge T(C^c \wedge B^{b1})\\
	&W > T(B^{b1} \wedge B^{b2}) (since b1 and b2 are in different sides w.r.t. W)\\
	&\implies T((A^a \vee B^{b1}) \wedge (C^c \vee B^{b2})) < W \text{ (q.e.d)}
\end{align*}

\paragraph{Theorem:} If S \(\vdash_{resolution}\) NULL \(\implies\) S is unsatisfiable.\\
\indent Proof: Let S1 be the set of all clauses after doing resolution on S. We have \(null \in S1\), so
S1 is unsatisfiable. According to Lemma 1, S must also be unsatisfiable (q.e.d).\\
\subsection{Proof of Completeness}
Proof of completeness means that we have to prove: \(KB \models G \implies KB {\vdash}_{resolution} G\). To put in English: the resolution method can derive G from KB if G is satisfiable in every model of 
KB. We can instead prove that if \(S = KB \cup \neg G\) is unsatisfiable,  doing resolution on
S derives the null clause.\\

\subsubsection{Proof by model constructing}
\paragraph{Well-founded ordering:} a binary relation \(<\) on a set S is a well-founded ordering if:
\begin{itemize}
	\item non-reflexive
	\item transitive
	\item there exists a least element in (S, \(<\))
\end{itemize}
\paragraph{Transfinite induction:} if \(B \subset A, A\) is well-founded ordered by \(<\), for every a in A:\\
\(({\forall c \in A, c < a} \subset B \implies a \in B) \implies A = B\)\\
\(\to\) if for some property P, \(\forall y < x, P(y) = true \implies P(x) = true\), then P is true for 
every x in A.
We can see transfinite induction is a generalization of the more familiar induction on the set of integer.\\
\paragraph{Proof of completeness}:\\
{\bfseries Lemma 2: If S is the transitive closure of resolution and S doesn't contain NULL, then S is
satisfiable\\}
Proof: We will prove this by constructing a model of S. Let I be an arbitrary interpretation of S, J be
our being constructed model of S.\\
Let False(I) be the set of false clauses of S under I, FalseL(I) the set of false literals of S under I.
Similarly we have True(I) and TrueL(I).
Let Y be the minimal cover of False(I), meaning that for all C in False(I), it contains at least one 
literal in Y, and Y is the smallest possible set satisfies that. It is trivial to see that for each 
literal in C there exists a clause C in False(I) that L is the only literal that both C and Y contain
(1).\\

Let's construct J such that:
\begin{itemize}
	\item \(T_J(L) \ge W \) if \(L \in Y\)
	\item \(T_J(L) \ge W \) if \(L \in TrueL(I)\) and there is no complement of L in Y
	\item \(T_J(L) < W\) otherwise
\end{itemize}

Now let \(<_1\) be a binary relation on S such that:\\
\(R <_1 C \iff R\) is a resolution of \(C\) and \(C_0 \in False(I)\). \(<_1\) is a 
well-founded ordering.\\
\\
We have to prove that: for all C in S, \(\forall (R <_1 C, T_J(R) \ge W) \to T_J(C) \ge W\).\\
If C is in False(I) then we always have \(T_J(C) \ge W\). If C is in True(I), assume that 
\(T_J(C) < W, \to \forall\) literals in C \(\in FalseL(J)\). C in True(I) so there exists a
literal \(L \in TrueL(I) \cap FalseL(J), \to \exists \text{the complement} L_1\) of L. \(L_1 \in
FalseL(I) \cap TrueL(J) \equiv Y\). From (1), we have C' being the clause in False(I) that has only 
\(L_1\) as a literal from Y \(\to L_1\) is the only true literal in C'. Doing resolution on C and C',
we have \(R1 = (C - L) \cup (C' - L1) \to R1\) contains only false literal under J. We also have R1
\(<_1\) C, so R1 must be true under J. So the assumption that \(T_J(C) < W\) is false, and \(T_J(C) 
\ge W\). Applying transfinite induction, we have \(T_J(C) \ge W\) in S. (q.e.d)\\

Applying Lemma 2, we have that for an unsatisfiable set of clauses S, resolution will always derive the
null clause. So the resolution inference rule is complete. (q.e.d)\\\\
\subsubsection{Proof by semantic tree}
\paragraph{Semantic tree: } Given a set of clauses S containing n symbols \(A_1..A_n\). The semantic
tree if S is a n-depth complete binary tree, each level corresponding to a literal. Children branches
of a node is labeled \(A_i < W\) and \(A_i > W\). 
\(\implies\) The semantic tree represents every interpretation possible for S, with each branch from
root to a leaf corresponding to an interpretation. The trivial case of \(A_i = W\) is not considered.\\\\
\xytree{
	%&&\xynode[-2,-1,0,1,2]{S}\\
	&&&&&&\xynode{} \xyconnect{1,-2}"_{\(A_{1} < W\)}" \xyconnect{1,2}"^{\(A_{1} > W\)}"\\
	&&&&\xynode{} \xyconnect{1,-1}"_{\(A_{2} < W\)}" \xyconnect{1,1}"^{\(A_{2} > W\)}" 
	&&&& \xynode{} \xyconnect{1,-1}"_{\(A_{2} < W\)}" \xyconnect{1,1}"^{\(A_{2} > W\)}"\\ 
	&&&\xynode{} && \xynode{} && \xynode{} && \xynode{}\\ 
}
\begin{figure}[h]
\caption{Semantic tree example}
\end{figure}
\\
\paragraph{Failure node: } A node is a failure node if there exists an interpretation corresponding to
the path from root to that node that S is false under that interpretation, and S is not false at 
any ancestor of that node.\\
\paragraph{Failure tree: } If every path from root to leaf contains a failure node, cut off the children
of those failure nodes, we get a failure tree.\\
\paragraph{Inference node: } A node in failure tree where both of its children are failure node.\\
\(\implies\) S is unsatisfiable if and only if it has a failure tree (1)\\
\(\implies\) There exists at least one inference node in a failure tree (2)\\
If a failure tree contains only one node, then it is false for the null interpretation.
Only null clause is false for null interpretation, so if S's failure tree contains only one node, S
must contain the null clause (3)\\
%
\paragraph{Proof of completeness: } Now we are going to prove that if \(S = KB \cup \neg G\) is 
unsatisfiable, resolution will derive the null clause from S. \\
According to (1), S has a failure tree. According to 2, S has at least one inference node.\\

\xytree{
	%&&\xynode[-2,-1,0,1,2]{S}\\
	&&&&&&\xynode{i} \xyconnect{1,-2}"_{\(A_{m} < W\)}" \xyconnect{1,2}"^{\(A_{m} > W\)}"\\
	&&&&\xynode{j} &&&& \xynode{k} \\
}

Let i be an inference node with j and k are its children. Because S is false at j where \(A_m < W\),
there must be a false clause C1 in S in the form \(...\vee {A_{m}}^{a1}, a1 > W\) and every other 
literal in C1 must be false at i. Similarly, there exist a C2 in S that false at k, and it has the form 
\(\cdots\vee {A_{m}}^{a2}, a2 > W\), every other literal in C2 must also be false at i.
Doing resolution on C1 and C2, we get C which contains every false literal in C1, C2 but \(A_{m}\).
Then we have \(S \vee C\)'s failure tree is S's failure tree cutting off j and k.\\
Repeat this process until there is only one node left. Now we have the transitive closure of resolution
on S, and this closure contain the null clause according to (3) (q.e.d).
%
\section{Refined hedge algebra}
\subsection{Distributive Lattice}
\paragraph{Partially ordered set: } A set P with an ordering binary relation that is reflexive, 
anti-symmetrical and transitive on it is called a partially ordered set, or poset.\\\\
Let a and b be two elements of the set. If \(a \le b\) or \(b \le a\), then a and b are {\em comparable}
, else they are incomparable. A set P is a totally ordered set if every two elements of P are comparable
to each other. A totally ordered set is also called a chain. An ordered set without any pair of 
comparable elements is called an antichain.\\\\
\paragraph{Lattice: } a poset (P, \(\le\)) is called a lattice if it satisfies two axioms:
\begin{itemize}
	\item \(\forall\) pair of elements a,b \(\in L\), \{a,b\} has {\em join}: \(a \vee b\) being the
		least upper bound (supremum) of \{a,b\}.
	\item \(\forall\) pair of elements a,b \(\in L\), \{a,b\} has {\em meet}: \(a \wedge b\) being 
		the greatest lower bound (infimum) of \{a,b\}.
\end{itemize}

Meet and join are binary operation on L. A lattice is called a bounded lattice if it has a greatest 
element and a least element, denoted 1 and 0 respectively. A lattice can be transformed into a bounded
lattice by adding to the set the least element \(0 = \wedge_{a_i \in L} a_i\) and the greatest element
\(1 = \vee_{a_i \in L} a_i\).
\paragraph{Properties of lattice: } Let the lattice L have a, b, c as its arbitrary elements:
\begin{itemize}
	\item Commutative: \(a \wedge b = b \wedge a, a \vee b = b \vee a\)
	\item Associative: \(a \vee (b \vee c) = (a \vee b) \vee c \), 
			\(a \wedge (b \wedge c) = (a \wedge b) \wedge c \)
	\item Absorbtion: \(a \wedge (a \vee b) = a\), \(a \vee (a \wedge b) = a\)
	\item Idempotent: \(a \wedge a = a\), \(a \vee a = a\)
\end{itemize}

We can treat the lattice (L, \(\le\)) as an algebraic structure with two operation meet and join: (L,
\(\wedge, \vee\)).\\\\

\paragraph{Distributive lattice: } A lattice (L, \(\wedge, \vee\)) is a distributive lattice if \(
\forall a,b,c \in L\): \\
\[ a \wedge (b \vee c) = (a \wedge b) \vee (a \wedge c)\\
a \vee (b \wedge c) = (a \vee b) \wedge (a \vee c)\\\]
\\
\paragraph{Modular lattice: } A lattice is a modular lattice if: \(x \le b \implies x \vee (a \wedge b)
= (x \vee a) \wedge b\).\\\\

\subsection{Refined-HA}
\paragraph{Semantical consistency: } let \( AX = (X,G,O,\le)\) be an arbitrary algebraic structure. 
Assume O is partitioned into two seperated subsets \( O^+, O^-\) such that \(O^+ + I, O^- + I\) are 
finite lattice with I being their zero element. Then X and O are semantically consistent if:
\begin{itemize}
	\item X is generated from elements in G under the effect of hedges in O i.e for \(x \in X, 
		x = h_1..h_na, h_i \in O, i = 1..n, a \in G\)
	\item \(\forall h,k \in O^c+I (c = + \text{ or } -), h<k in O^c+I \iff \forall x \in X:
		hx > x \text{ or } kx > x \implies hx < kx, \text{ and } hx < x \text{ or } kx <x
		\implies kx < hx\). h, k are incomparable in \(O^c+I \iff \forall (x \in X, hx \neq x
		\text{ or } kx \neq x) \implies\) hx and kx are incomparable.\\
\end{itemize}
%
\paragraph{Cover: } let P be a poset. An element a is called of cover of another element b if
\(b < a \text{ and } \not \exists x \in P, b < x < a\)\\
Let l(P) denote the length of a poset P. If P has finite length and has an infimum element 0, 
the height of an element x in P is denoted by height(x), which is the least upper bound of the length of
the chain \(0 = x_0<x_1..<x_n = x\) between 0 and x. If P has a supremum 1, it is obvious that height(1)
= l(P), and height(x) = 1 if x cover 0.\\\\

\paragraph{Graded poset: } a poset P can be graded if there exists a function \(g:P\to Z\) such that:
\begin{itemize}
	\item \(x > y \implies g(x) > g(y) \)
	\item if x cover y then \(g(x) = g(y) + 1\)
\end{itemize}
Then g is a rank function of P. A finite modular lattice is graded by its height function.\\
Let L be a finite modular lattice, we define the relation R on L such that: \(\forall x,y \in L, (x,y)
\in R \iff height(x) = height(y)\)\\
R is then an equivalence relation on L. We can partition L into equivalence classes: 
\(L = \overset{l(L)}{\underset{i=0}{\bigcup}} L_i\)\\
Each class \(L_i\) corresponding to a height i, which means every element in \(L_i\) has the height of i.\\
To be able to operate on linguistic hedges, we also set the follow condition:\\
\indent \((C_0)\) For all \(x \in L_i, y \in L_j, i \neq j\) then \(x>y\) or \(x<y\)
\paragraph{Theorem: } let L be a finite modular lattice and satisfies \(C_0\). Then: if \(|L_i| > 1\)
 with \(i \in \{1..l(L)-1\}\) then \(|L_{i-1}| = 1\) and \(|L_{i+1}| = 1\).\\
Let H be the set of hedges, such that \(H^+ + I\) and \(H^- + I\) are finite modular lattice
satisfying \(C_0\). Let \(N^+\) and  \(N^-\) be the length of \(H^+ +I\) and \(H^- + I\). \(g^+\) 
and \(g^-\) are rank functions of \(H^+ +I\) and \(H^-+I\). Let V and S be unit operation of \(H^+ +I\)
and \(H^-+I\). Then we have \(g^+(V) = N^+, g^-(S) = N^-\) and:\\
\begin{itemize}
	\item \(H^+ + I = \overset{N^+}{\underset{i=0}{\bigcup}} {H^+}_i, 
		\text{ where } {H^+}_i =\{ h \in H^+I| g^+(h) = i\}\)
	\item \(H^- + I = \overset{N^-}{\underset{i=0}{\bigcup}} {H^-}_i, 
		\text{ where } {H^-}_i =\{ h \in H^-I| g^-(h) = i\}\)
\end{itemize}
Let \({LH^+}_i = (L({H^+}_i), \wedge, \vee)\) be a lattice free-generated from incomparable elements
\({h^i}_1..{h^i}_n \in {H^+}_i\). With index i such that \(|{H^+}_i| = 1\) we have \({LH^+}_i = 
{H^+}_i\). Let \(LH^+ = {\bigcup^{N^+}}_{i=1} {LH^+}_i\) and \({LH^+} + I = LH^+ \cup \{I\}
= {\bigcup^{N^+}}_{i=0} {LH^+}_i\). Then \({LH^+} + I\) is a distributive lattice with the ordering
relation being the ordering relation on the lattice \({LH^+}_i\), and \({LH^+}_i\) is a graded class
of \(LH^+\).\\
Similarly we can construct \(LH^- + I\) from \(H^- + I\). Because \(H^+ \not{\cap} H^-\), \(LH^+ \not{
\cap} LH^-\). We have the following theorem:\\\\
\paragraph{Theorem: } \((LH^+ + I, \vee, \wedge,I,V,\le)\) and \((LH^- + I, \vee, \wedge,I,S,\le)\)
are finite distributive lattice  with unit operation V and S, infimum element I.\\
\paragraph{Notation: } Before investigate the definition of refined hedge algebra, we now introduce
somw notation: \\
\begin{itemize}
	\item \(LH^*\) is the set of all hedge strings generated from LH
	\item UOS the unit operation set
	\item \({LH^c}_i[x] = \{hx|h \in {LH^c}_i\}\)
	\item LH(x) is the set of all linguistic value \(\alpha x, \alpha \in LH^*\)
\end{itemize}
We now have the definition of refined hedge algebra:\\\\
\paragraph{Refined hedge algebra: } \(AX = (X,G,LH,\le)\) is a refined hedge algebra if X and LH are
semantically consistent and AX satisfies the following conditions:\\
\begin{itemize}
	\item Every operation in \(LH^-\) is converse to every operation in \(LH^+\)
	\item unit operation of \(H^++I (H^-+I) \) is positive (negative) w.r.t. every element in H
	\item if u and v are independent (\( u \notin LH(v), v \notin LH(u)\)) then \(x \notin LH(u)\) 
	for every x in LH(v) and vice versa. If \(x \neq hx, x\notin LH(hx)\). If \(kx \neq hx\) then kx
	and hx are independent.
	\item Semantic heredity - comparability preservation: if hx and kx are incomparable, then \(
	\forall u \in LH(hx), v \in LH(kx)\), u and v are incomparable. If \(a,b \in G\) and \(a<b\)
	then \(LH(a) < LH(b)\). If \(hx < kx\):\\
	\begin{itemize}
		\item if \(h,k \in {LH^c}_i, i \in SI^c\):
			\begin{itemize}
				\item \(\delta hx < \delta kx, \forall \delta in LH^*\)
				\item \(\delta hx\) and y are incomparable with all \(y \in LH(kx)\)
					such that \(y \not> \delta kx\)
				\item \(delta kx\) and z are incomparable, for all \(z \in LH(hx)\)
					such that \(z \not< delta hx\)
			\end{itemize}
		\item if \(\{h,k\} \not\subset {LH^c}_i \forall i \in SI^c\) or hx = kx, then 
			\(h'hx \le k'kx\) for all h',k' in UOS.
	\end{itemize}
	\item (Linear ordering between graded-term sets): Assume that \(u \in LH(x), u\notin 
		LH({LH^c}_i[hx]) \forall i \in I^c\) If there exists \(v \in LH(hx)\) with 
		\(h \in {LH^c}_i\) such that \(u \ge v\) or \(u \le v\), then \(u \ge h'v\) 
		(or \(u \le h'v\) for \(h' \in UOS\)	
\end{itemize}
As we can see, refined hedge algebra is a generalization of hedge algebra. We now have the following 
theorem:\\
\paragraph{Theorem: } let \(AX = (X,G,LH,\le)\) be a refined hedge algebra. if G is a chain then AX is
a distributive lattice.\\
With AX being a distributive lattice, we can define intersection and union operations as AX's meet and 
join, and preserve every properties of hedge algebra's intersection and union operations. We then define
negation on RHA:\\
\paragraph{Negation: } let \(AX = (X,G,LH,\le\) be a refined hedge algebra with \(G = \{
0,f,W,t,1\}\). Let \(x = h_1..h_n a\) with \(a \in \{t,f\}\), then y is the negation of x, denoted 
y = -x if \(y = h_1..h_n a', a' \in \{t,f\}, a \neq a'\).\\
\paragraph{Refined linear symmetrical hedge algebra: } let \(AX = (X,G,LH,\le)\) be a refined hedge
algebra, with \(G = \{0,f,W,t,1\}\). AX is called symmetrical if every element of X has one and only one
 negation.\\
Noted that with every important properties that makes the propositional logic using linear symmetrical
hedge algebra in previous sections able to reason using resolution rule, it is certainly possible to 
use refined hedge algebra as a basis for a similar logic that also capable of reasoning using resolution
.\\\\
\section{Conclusion and research direction:}
We now reached the end of this report. In this semester, I have done some researched on hedge algebra and
automated reasoning using resolution on a propositional fuzzy logic with hedge algebra as the domain 
of truth value, and take a peek at the more general refined hedge algebra. \\\\
On the one hand, that also means there are still many unsolved problem, for instance there is still no way to 
compute the truth value under an interpretation of a term in our logic, or we have only investigated
propositional logic, leaving predicate logic untouched. \\\\
On the other hand, there are also many feasible possibilities lie beyond what described in this report,
for particular the potential of refined hedge algebra in logic and automated reasoning, or how to 
designing a logic programming language using a predicate fuzzy logic based on hedge algebra. 
%And lastly,
%it might sound too farfetched now, but I have an intuition that there might exist a constructive
 %but also fuzzy logic based on hedge algebra, which according to Curry-Howard isomorphism, 
%is corresponding to a type system of a programming language. Since constructive logic can be 
%an infinitely many-valued logic, which fuzzy logic based on hedge algebra can represent, so it does
%seem possible.\\
\\\\
This concluded the report of my graduation research on Linguistic Logic Reasoning this semester.
Hopefully this will serve as a good basis for my future works on this thesis theme. 
\end {document}
